{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# IMDB 数据集，包含来自互联网电影数据库（IMDB）的50000条严重两极分化的评论\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数 num_words = 10000 的意思是仅保留训练数据中前 10000 个最常出现的单词\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict(\n",
    "    [(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# 索引减去3，是因为 0、1、2 是为“padding”、“start of sequence”、“unknown”分别保留的索引\n",
    "decoded_review = ' '.join(\n",
    "    [reverse_word_index.get(i-3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n",
    "print(decoded_review[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据，在输入网络之前要先调整数据的维数为指定的长度\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension = 10000):\n",
    "    \"\"\"\n",
    "    将sequence数据调整为10000维的长度, one_hot 形式\n",
    "    \"\"\"\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)     \n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建网络进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "\"\"\"\n",
    "使用Sequential 类构建模型，仅用于层的线性堆叠，\n",
    "这是目前最常见的网络架构。\n",
    "\"\"\"\n",
    "model = models.Sequential() \n",
    "# input_shape = (10000,) 说明是以单个样本作为输入数据，并不是一个 batch 作为输入\n",
    "model.add(layers.Dense(16, activation = 'relu', input_shape = (10000, )))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "\"\"\"\n",
    "上述构建网络的过程也可以通过函数式api的调用来实现\n",
    "input_tensor = layers.Input(shape = (10000,))\n",
    "x = layers.Dense(16, activation = 'relu')(input_tensor)\n",
    "x = layers.Dense(16, activation = 'relu')(x)\n",
    "output_tensor = layers.Dense(1, activation = 'sigmod')(x)\n",
    "model = models.Model(inputs = input_tensor, outputs = output_tensor)\n",
    "\"\"\"\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', \n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据划分成验证集和训练集\n",
    "x_val = x_train[:10000]\n",
    "y_val = train_labels[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "partial_y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 4s 289us/step - loss: 0.5223 - acc: 0.7747 - val_loss: 0.3959 - val_acc: 0.8592\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 2s 130us/step - loss: 0.3142 - acc: 0.9003 - val_loss: 0.3118 - val_acc: 0.8844\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 2s 130us/step - loss: 0.2307 - acc: 0.9255 - val_loss: 0.3005 - val_acc: 0.8809\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 2s 130us/step - loss: 0.1817 - acc: 0.9428 - val_loss: 0.2791 - val_acc: 0.8877\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 2s 131us/step - loss: 0.1481 - acc: 0.9544 - val_loss: 0.2772 - val_acc: 0.8876\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 2s 131us/step - loss: 0.1212 - acc: 0.9635 - val_loss: 0.3080 - val_acc: 0.8804\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 2s 130us/step - loss: 0.0991 - acc: 0.9727 - val_loss: 0.3032 - val_acc: 0.8844\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 2s 131us/step - loss: 0.0817 - acc: 0.9773 - val_loss: 0.3296 - val_acc: 0.8790\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 2s 129us/step - loss: 0.0660 - acc: 0.9823 - val_loss: 0.3471 - val_acc: 0.8771\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 2s 130us/step - loss: 0.0530 - acc: 0.9877 - val_loss: 0.3684 - val_acc: 0.8784\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 2s 131us/step - loss: 0.0443 - acc: 0.9899 - val_loss: 0.3928 - val_acc: 0.8752\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 2s 131us/step - loss: 0.0324 - acc: 0.9939 - val_loss: 0.4527 - val_acc: 0.8660\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 2s 130us/step - loss: 0.0273 - acc: 0.9944 - val_loss: 0.4544 - val_acc: 0.8771\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 2s 128us/step - loss: 0.0211 - acc: 0.9966 - val_loss: 0.4868 - val_acc: 0.8713\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 2s 130us/step - loss: 0.0177 - acc: 0.9966 - val_loss: 0.5187 - val_acc: 0.8690\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 2s 131us/step - loss: 0.0096 - acc: 0.9993 - val_loss: 0.5492 - val_acc: 0.8711\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 2s 130us/step - loss: 0.0113 - acc: 0.9984 - val_loss: 0.5870 - val_acc: 0.8660\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 2s 130us/step - loss: 0.0085 - acc: 0.9989 - val_loss: 0.6025 - val_acc: 0.8698\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 2s 130us/step - loss: 0.0042 - acc: 0.9999 - val_loss: 0.6361 - val_acc: 0.8696\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 2s 130us/step - loss: 0.0065 - acc: 0.9984 - val_loss: 0.6650 - val_acc: 0.8679\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs = 20,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [0.39587247262001035, 0.31181207814216616, 0.3005114956855774, 0.279057532787323, 0.2772214065551758, 0.30799101595878603, 0.3032468152999878, 0.3295767570495605, 0.347121275472641, 0.3683795490264893, 0.39280004549026487, 0.4526786060333252, 0.4543756044387817, 0.4868070682525635, 0.5186537066936493, 0.5491567985534668, 0.5869648418426514, 0.6025346612930298, 0.6360756613731384, 0.6650033417701722], 'val_acc': [0.8592000007629395, 0.8843999998092651, 0.8809000002861023, 0.8877000001907349, 0.8876, 0.8804000001907348, 0.8844, 0.8789999998092651, 0.8770999997138977, 0.8784000000953675, 0.8751999998092651, 0.8659999998092651, 0.8770999997138977, 0.8712999996185302, 0.8689999997138977, 0.871099999332428, 0.8659999995231629, 0.8697999998092651, 0.8695999998092652, 0.8678999999046325], 'loss': [0.5223330561478933, 0.3142386782010396, 0.2307170506954193, 0.18165733381907145, 0.14810868453979492, 0.1212099373737971, 0.09913595113356909, 0.08167822510401408, 0.06598797802527745, 0.053041776659091315, 0.044328639471530915, 0.03244292518297831, 0.027277365244428316, 0.02105742723941803, 0.017652649400631586, 0.00956783381998539, 0.011336499704420567, 0.008461985803147156, 0.004219711889574925, 0.006450156062593063], 'acc': [0.7747333334287008, 0.9002666667938233, 0.9255333336194357, 0.9427999998092651, 0.9544000000635783, 0.9635333335558574, 0.9727333334922791, 0.9773333335876465, 0.9822666668256124, 0.9876666666666667, 0.9899333334922791, 0.9939333335876465, 0.9944000002543132, 0.9966, 0.9966000002543132, 0.9993333333333333, 0.9984000002543132, 0.9989333333333333, 0.9998666666666667, 0.9984]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
